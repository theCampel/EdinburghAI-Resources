{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you enjoyed the Intro to Machine Learning. Let's dive into **neural networks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is Deep Learning? #\n",
    "\n",
    "Okay, so if that's what machine learning is, what is **deep learning**? Deep learning is a kind of machine learning that takes inspiration from the brain. It takes simple **neurones** that communicate with each other to build a larger **neural network**. Despite each neurone being simple by itself, they combine to exhibit complex behaviour. Your brain üß† contains ~86 billion neurones, and you can do lots of complex things!\n",
    "\n",
    "If you want to understand why neural networks are so powerful from a mathematical perspective, you can read about the *Universal Approximation Theorem* [here ](https://towardsdatascience.com/neural-networks-and-the-universal-approximation-theorem-8a389a33d30a) or watch a short visualisation [here.](https://www.youtube.com/watch?v=Ln8pV1AXAgQ)\n",
    "\n",
    "Most of the hype around artificial intelligence in recent years has been in **deep learning**. Natural language tasks like translation, summarisation, chatbot generation or image generation and recognition are just some of the tasks where deep learning models have neared or even exceeded human-level performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is Deep Learning? #\n",
    "\n",
    "Okay, so if that's what machine learning is, what is **deep learning**? Deep learning is a kind of machine learning that takes inspiration from the brain. It takes simple **neurones** that communicate with each other to build a larger **neural network**. Despite each neurone being simple by itself, they combine to exhibit complex behaviour. Your brain üß† contains ~86 billion neurones, and you can do lots of complex things!\n",
    "\n",
    "If you want to understand why neural networks are so powerful from a mathematical perspective, you can read about the *Universal Approximation Theorem* [here ](https://towardsdatascience.com/neural-networks-and-the-universal-approximation-theorem-8a389a33d30a) or watch a short visualisation [here.](https://www.youtube.com/watch?v=Ln8pV1AXAgQ)\n",
    "\n",
    "Most of the hype around artificial intelligence in recent years has been in **deep learning**. Natural language tasks like translation, summarisation, chatbot generation or image generation and recognition are just some of the tasks where deep learning models have neared or even exceeded human-level performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Single Neurone ‚ö™\n",
    "\n",
    "A single neurone is just a straight line function. It has two **parameters**: its **weight, w,** and its **bias, b**. The input is multiplied by the weight, and added to the bias, and the result is passed forwards. \n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/mfOlDR6.png\" width=\"250\" alt=\"Diagram of a linear unit.\">\n",
    "\n",
    "</figure>\n",
    "\n",
    "Let's see how to make one in Pytorch üî®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=1, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[0.6375]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6713], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Let's import torch as well as the nn module\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# A layer of neurones with 1 input and 1 output (i.e. a single neuron)\n",
    "layer = nn.Linear(1, 1)\n",
    "\n",
    "# Let's print the neuron and its parameters\n",
    "print(layer)\n",
    "print(layer.weight)\n",
    "print(layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are just random values. If you want to know how these values are decided, you can check out [the docs](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). \n",
    "\n",
    "Try changing the parameters of the single neurone to have more than one input and output.\n",
    "**Thinkü§î**: What changed? What's the data type of `neuron.weight`? and `neuron.bias`? \n",
    "\n",
    "**Extension**: A bit tougher - any idea what `requires_grad` means? Why do you think it's set to `True`? Can you think of a reason you might want to set it to `False`?\n",
    "\n",
    "Now let's run a value through the neuron by simply 'calling' the object. You can call `.forward()`, but this is worse practice. We get the output as a *tensor*. Don't be scared - this is just a fancy name for a list with more than one dimension. We use this word like how we use the word matrix for 2D arrays, but tensors can have any number of dimensions. For a tensor containing one value, you can call `.item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0338], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ceate a tensor (fancy name for a multi-dimensional array) with a single value\n",
    "x = torch.tensor([1.0])\n",
    "\n",
    "# Let's apply the neuron to the tensor\n",
    "y = layer.forward(x)\n",
    "\n",
    "# Let's print the result\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers of Neurones\n",
    "\n",
    "A single layer isn't very cool. Let's add more layers.\n",
    "\n",
    "But first! - we can't just add layers upon layers of straight lines or elese we will only be making straight lines. We need an **activation function** to add some **non-linearity** to our model. We use `nn.ReLU` as it is the most common, but it is not the only choice.\n",
    "\n",
    "**Thinkü§î**: Can you think of other ways you might add non-linearity to your model (without changing the neuron)? \n",
    "\n",
    "*Hint: max(0, wx + b) is not the only function that make straight lines not straight anymore.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# We stack layers with torch.nn.Sequential\n",
    "# We can also add activation functions\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 1)\n",
    ")\n",
    "\n",
    "# Let's print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension**: Can you draw the model above? How many 'layers' are there really? How many parameters (weights and biases)? So how many 'neurones' are there? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training A Neural Network üéì\n",
    "\n",
    "### Loss Function üìâ \n",
    "\n",
    "To train a neural network, you need 2 things: a **loss function** and an **optimiser**.\n",
    "\n",
    "A loss function decides how good a job your model did. The lower it is the better. Through training, the loss should go down, indicating that the model is getting better at the task.\n",
    "\n",
    "For example, in the lecture we had an input of 1 and a desired output of 2. We ran the model and got an output of 0. We could score this with a loss function of:\n",
    "\n",
    "$\\text{Loss}(\\text{model output}, \\text{desired output}) = |\\text{model output} - \\text{desired output}|$\n",
    "\n",
    "or in more mathematical language:\n",
    "\n",
    "$\\text{L}(\\hat{y}, y) = |\\hat{y} - y|$ \n",
    "\n",
    "where $y$ is the desired output and $\\hat{y}$ is the model's guess\n",
    "\n",
    "**Thinkü§î**: Why is the absolute value there?\n",
    "\n",
    "*Hint: Imagine the absolute value wasn't there. What would happen if the model outputted 5 and you wanted it to output 10?*\n",
    "\n",
    "For a real world example, if we wanted our model to predict the next word, we could define a loss function that scores 0 if the next word is correct and 1 if it is incorrect. Minimising this would mean that the model always outputs the correct next word - ideal! This is not conceptually too far from what is actually done in practice.\n",
    "\n",
    "### Optimiser üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "An optimiser decides how to change the parameters of the model to do a better job next time. After the loss has been calculated, it goes in and updates the parameters in the model so that the model gets closer to the desired output. \n",
    "\n",
    "It does this by going backwards through the model, looking at each parameter, and seeing whether you should increase or decrease that parameter in order to make the loss smaller. \n",
    "\n",
    "The optimiser doesn't just look at the direction, it also looks at how much to change each parameter by. It does this by determining how sensitive the loss is to changes of each parameter. For math nerds: it does this using derivatives and the chain rule i.e. the gradient of the loss function w.r.t the parameter. It also multiplies this number by a *learning rate* which is generally set to somewhere between 0.01 and 0.00001, depending on the task, model and loss function. This is a *hyperparameter.* of our NN - it's set by you to guide the ML algorithm to do a better job.\n",
    "\n",
    "**Thinkü§î**: Why do you think these processes are called **gradient descent** and **backpropagation**? Why is it called a **learning rate**?\n",
    "\n",
    "**Thinkü§î**: What do you think changing the learning rate does? What happens if it's too small? Or too big?\n",
    "\n",
    "*[Hint: Check out this picture](https://media.licdn.com/dms/image/D5612AQHEVVxj-OS1og/article-cover_image-shrink_720_1280/0/1695927263310?e=2147483647&v=beta&t=XHFLMNaRVcMTx_EG8twpMJeZNf5dgINmbXmYLzBa49U)*\n",
    "\n",
    "**Extension**: How might you test what a good learning rate is for your specific task? How would you know? \n",
    "\n",
    "## Practical Side üî®\n",
    "\n",
    "Create \n",
    "- a NN\n",
    "- a loss function\n",
    "- an optimiser, here you choose the *learning rate*\n",
    "\n",
    "Train the model:\n",
    "- Reset the optimiser with `optimiser.zero_grad()`\n",
    "- Run some data through the model. \n",
    "- Calculate the loss\n",
    "- Backpropagate and calculate gradients with `loss.backward()`\n",
    "- Update model parameters with `optimiser.step()`\n",
    "- Repeat\n",
    "\n",
    "Done!\n",
    "\n",
    "Some lingo: passing all of your data through is called 1 **epoch**. For some kinds of data, this is enough. For other kinds of data, you will need many epochs (10s if not 100s!).\n",
    "\n",
    "Let's try it in a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single neuron\n",
    "import torch.optim\n",
    "\n",
    "neuron = nn.Linear(1, 1)\n",
    "\n",
    "# Create a simple custom loss function\n",
    "def custom_loss(y_pred, y_true):\n",
    "    return torch.abs(y_pred - y_true)\n",
    "\n",
    "# Create a optimizer, with a learning rate ('lr') of 0.01\n",
    "# We pass in the parameters of the neuron\n",
    "# SGD stands for Stochastic Gradient Descent.\n",
    "optimiser = torch.optim.SGD(neuron.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a model (a single neuron), a loss function and an optimiser we can train the model. You can run the following cell repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "y = 0.6x + 1.21\n",
      "Prediction: 1.81\n",
      "After training:\n",
      "y = 0.7x + 1.31\n",
      "Prediction: 2.01\n"
     ]
    }
   ],
   "source": [
    "# Reset the gradients\n",
    "optimiser.zero_grad()\n",
    "\n",
    "# Create an input tensor with two values\n",
    "x = torch.tensor([1.0])\n",
    "\n",
    "# Create an output tensor with one value\n",
    "y_true = torch.tensor([2.0])\n",
    "\n",
    "# Print the weight and bias and prediction before training\n",
    "print('Before training:')\n",
    "print(f'y = {round(neuron.weight.item(), 2)}x + {round(neuron.bias.item(), 2)}')\n",
    "# Compute the prediction\n",
    "y_pred = neuron.forward(x)\n",
    "\n",
    "print(f'Prediction: {round(y_pred.item(), 2)}')\n",
    "\n",
    "# Compute the loss\n",
    "loss = custom_loss(y_pred, y_true)\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update the weights\n",
    "optimiser.step()\n",
    "\n",
    "# Print the updated weight and bias after training\n",
    "print('After training:')\n",
    "print(f'y = {round(neuron.weight.item(), 2)}x + {round(neuron.bias.item(), 2)}')\n",
    "\n",
    "# Print the updated prediction\n",
    "y_pred = neuron.forward(x)\n",
    "\n",
    "print(f'Prediction: {round(y_pred.item(), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thinkü§î**: Can you see what's going on here? Try setting different learning rates and seeing what happens.\n",
    "\n",
    "**Extension**: Try changing the data between training runs to make the model fit the straight line $y=10x + 5$.\n",
    "\n",
    "Repeatedly running is a bit tiresome. Generally, we loop over our dataset in a for loop, as you'll see below.\n",
    "\n",
    "Slightly more advanced detail, we also *batch* our inputs. This means running several values at once through the model. We then *accumulate* the gradients, and update the parameters for those data points all at once. This is mostly for efficiency's sake, which is also the reason we don't generally run the entire datset at once through the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You First Neural Network! üî•\n",
    "\n",
    "You're now ready for a real problem. We're going to tackle the Hello Wordl of Neural Networks: The MNIST dataset. This is a dataset of handwritten digits, and the model should take in the image and detect which digit this is.\n",
    "\n",
    "So let's follow the plan. We'll start with a model, loss function and optimiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement MNIST classification with a neural network\n",
    "\n",
    "# This is an 'Adam' optimizer, which is a variant of the stochastic gradient descent. If you want to know more about it, check out the paper: https://arxiv.org/abs/1412.6980, but it's basically a fancy version of gradient descent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
